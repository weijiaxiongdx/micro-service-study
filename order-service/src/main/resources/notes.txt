1.项目搭建过程
  1.1登录github官网，创建仓库并获取仓库地址https://github.com/weijiaxiongdx/micro-service-study.git
  1.2进入D盘，右键->Git Clone，把项目clone到本地
  1.3新建order-service、goods-service模块，右键项目名称->New->Module->Java->Module SDK->选择java版本
  1.4order-service、goods-service分别手动新建目录、包、pom.xml文件、启动类等(同名目录、同名包会自动分层)。
     如果1.3中新建模块时选择的是Maven，则不用手动创建目录结构、pom.xml文件等，Maven帮我们创建好了
  1.5修改配置文件，配置端口等信息。pom.xml引入依赖
  1.6设置注释模板快捷键(ctrl+alt+s可直接打开Setting界面)
     File->Setting->Live Templates->点击右上角"+"号，新增Live Template
     Abbreviation输入项表示快捷键字母，Description输入项表示对该快捷键的描述，Expand with下拉选项选择与字母组合的键(在类上按c+Enter可生成类注释)，最下方的change配置
     Template text输入项表示模板内容(需要配置日期、时间、包名、类名等变量)，
       类模板如下(在类上按c+Enter可生成类注释)
       /**
        * @Desc: $Desc$
        * @File name：$PACKAGE_NAME$.$NAME$
        * @Create on：$DATE$ $TIME$
        * @Author：wjx
        */

        方法模板如下(在方法上按x+Enter可生成方法注释)
        /**
         * @Des $des$
         * @Date $date$ $time$
         * @Param $param$
         * @Return $return$
         * @Author wjx
         */
  1.7搭建nacos服务
     1.7.1下载nacos，选择对应版本及对应平台的文件包(下载此时最新版本1.4.3的nacos-server-1.4.3.zip文件) https://github.com/alibaba/nacos/releases
     1.7.2解压并启动nacos服务,此版本nacos默认以集群方式启动，先用单机玩玩，否则直接启动会报错
         方式一：修改startup.cmd文件中的set MODE="cluster"为set MODE="standalone"，直接双击startup.cmd文件即可启动
         方式二：进入bin目录，执行命令startup.cmd -m standalone即可启动
     1.7.3访问(默认用户和密码都是nacos)http://127.0.0.1:8848/nacos/index.html

     1.7.4阿里云服务器安装nacos(主机120.79.89.16，账号/密码root/711454466.cn)
          1.7.4.1进入目录，没有则创建
                 /usr/local/nacos
          1.7.4.2下载nacos安装包nacos-server-1.4.3.tar
                  方式一，在服务器当前目录下执行以下命令，太慢了，10分钟才下载了百分之十几，11:20开始下载，大概三四十分钟才下载完成
                        https://github.com/alibaba/nacos/releases/download/1.4.3/nacos-server-1.4.3.tar.gz
                  方式二，直接本地下载安装包并上传到服务器目录/usr/local/nacos
                         https://github.com/alibaba/nacos/releases

                         上传前需要先安装对应的命令yum -y install lrzsz
                         通过rz命令上传一直报错，最后通过WinSCP工具上传成功
          1.7.4.3解压安装包
                 tar -zxvf nacos-server-1.4.3.tar.gz
          1.7.4.4修改/usr/local/nacos/bin目录下的启动脚本文件startup.sh
                 将[ ! -e "$JAVA_HOME/bin/java" ] && JAVA_HOME=$HOME/jdk/java改成如下
                   [ ! -e "$JAVA_HOME/bin/java" ] && JAVA_HOME=/usr/local/jdk/jdk18
          1.7.4.5单机方式启动nacos
                   sh startup.sh -m standalone
          1.7.4.6在阿里云ESC服务器配置安全组，以开放8848端口，供外网访问
                 http://120.79.89.16:8848/nacos/index.html
          1.7.4.6修改端口号(如把端口修改为8858)
                 1.7.4.6.1阿里云服务器管理控制台配置安全组
                 1.7.4.6.2开放端口firewall-cmd --permanent --add-port=8858/tcp
                          重新加载防火墙配置firewall-cmd --reload
                 1.7.4.6.3修改配置文件/usr/local/nacos/nacos/conf/application.properties中的端口信息
                          server.port=8858
                 1.7.4.6.4重新启动nacos(先停掉服务)
                          sh startup.sh -m standalone
                 1.7.4.6.5浏览器访问
                          http://120.79.89.16:8858/nacos/index.html

  1.8应用集成nacos
     1.8.1引入依赖spring-cloud-starter-alibaba-nacos-discovery
     1.8.2启动类上加注解@EnableDiscoveryClient
     1.8.3配置文件中配置nacos服务的地址，服务名称也必须配置，否则服务不能注册到nacos
     1.8.4应用启动成功后，nacos服务列表查看应用是否注册成功
  1.9应用集成Feign进行远程服务调用
     1.9.1引入依赖spring-cloud-starter-openfeign
     1.9.2启动类上加注解@EnableFeignClients
     1.9.3创建接口GoodsServiceFeign
     1.9.4遇到的问题，引入openfeign依赖后，远程调用失败，因为spring-cloud-starter-openfeign与spring-cloud-starter-alibaba-nacos-discovery
          使用的版本不兼容，单独引入archaius-core依赖即可解决问题。也可找二者兼容的版本。
  2.0应用集成sentinel
     2.0.1引入依赖spring-cloud-starter-alibaba-sentinel
     2.0.2下载sentinel-dashboard，访问https://github.com/alibaba/Sentinel/releases，下载jar包sentinel-dashboard-1.8.3.jar(此时的最新版本)
     2.0.3启动sentinel-dashboard，执行以下命令
          java -Dserver.port=8080 -Dcsp.sentinel.dashboard.server=localhost:8080 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard-1.8.3.jar
     2.0.4登录sentinel-dashboard(默认用户名和密码都是sentinel)，http://localhost:8080，此时只能看到sentinel-dashboard本身一个服务
     2.0.5配置文件增加sentinel的相关配置
     2.0.6重启应用，刷新sentinel-dashboard控制台页面，发现还是只有sentinel-dashboard本身一个服务。这是因为sentinel-dashboard是懒加载的，
          此时只要访问一下order-service应用(调一下接口)，sentinel-dashboard控制台就可以看到order-service服务了
     2.0.7sentinel-dashboard控制台配置流控规则，在"簇点链路"页面找到对应的接口资源(以下规则配置默认是存储在对应的微服务内存中，在控制台配置规则并保存后，
                            把规则推送到了Sentinel客户端，即推送到了对应的微服务，重启微服务就没了)
          流控规则： 针对来源->针对哪个微服务的调用来作流量控制，
                   阈值类型->阈值类型配置为QPS、单机阈值配置为3，则表示每秒只能处理3个请求，超过3个请求就会被限流，默认提示为“Blocked by Sentinel (flow limiting)”
                           阈值类型配置为并发线程数、单机阈值配置为3，则表示只能同时支持3个线程的并发请求(用jmeter模拟多个线程访问，默认提示同上)
                   流控模式->直接表示配置的资源名到达阈值时，会被限流
                           关联表示配置的关联资源到达阈值时，会对配置的资源名进行限流，可做请求让步(查询让步更新)
                           链路表示配置的入口资源到达阈值时，会对配置的资源名进行限流，例如C为配置的资源名，A、B都调用C，A同时配置为链路的入口资源，那么A到达阈值时，C会被限流
                   流控效果->快速失败表示请求数超过阈值时，直接抛异常，默认就是此效果，默认异常提示为“Blocked by Sentinel (flow limiting)”
                            Warm Up(预热)，一开始的阈值为最大阈值的1/3，然后慢慢增长，直到最大阈值，适用于将突然增大的流量转换为缓步增长的流量的场景
                            排队等候，超过阈值的请求会进入队列等候，超过配置的等候时间则视为失败
          熔断规则： 熔断策略->慢调用比例，平均响应时间超过配置的阈值时，则触发熔断
                           异常比例，单位时间内异常请求占比超过阈值时，则触发熔断
                           异常数，单位时间内异常请求数超过阈值时，则触发熔断
          热点规则： 将流控规则应用到参数级别(参数名称、甚至到参数值)
          授权规则： (不)允许授权中配置的某(些)应用访问配置的资源名。比如服务A、服务B都调用了服务C，那么在服务C中可以配置授权规则，指定服务A(不)可以访问自己
          系统规则： 应用级别的限流，监控的是单台机器入口流量。上面4个都资源级别的限流
     2.0.8自定义各规则异常返回，配置BlockExceptionHandler处理组件，针对不同的异常作不同的处理
          这种异常处理优先级高于@SentinelResource注解配置的降级逻辑
     2.0.9规则持久化 todo
  2.1Feign整合Sentinel实现容错处理
     2.1.1引入依赖spring-cloud-starter-alibaba-sentinel，前面步骤已引入
     2.1.2开启Feign对Sentinel的支持，在配置文件中配置feign.sentinel.enabled=true
     2.1.3创建容错类，然后通过@FeignClient注解中的fallback属性指定容错类(拿不到远程调用异常信息)，或通过fallbackFactory属性指定容错类(可以拿到远程调用异常)
     2.1.4遇到的问题，启动报错com.alibaba.cloud.sentinel.feign.SentinelContractHolder.parseAndValidatateMetadata
          将spring-cloud-starter-openfeign依赖的版本升级到2.2.2.RELEASE即可解决
     2.1.5测试，停掉商品服务或在代码中人为制造异常，访问订单服务
  2.2引入网关(以Spring Cloud Gateway网关为例)
     2.2.1业界流行网关Ngnix+lua、Kong(基于ngnix+lua开发)、Zuul、Spring Cloud Gateway
          Spring Cloud Gateway网关缺点：基于Netty和WebFlux开发，学习成本高；不能部署在Tomcat、Jetty等Servlet容器，只能打成jar包执行；Spring boot2.0及以上版本才支持
     2.2.2新建api-gateway网关模块、引入依赖spring-cloud-starter-gateway、创建启动类
     2.2.3配置文件中配置cloud.gateway.routes相关配置
     2.2.4浏览器访问http://localhost:8110/order-serv/order/detail?orderId=310，请求成功转发到订单服务
  2.3网关服务集成nacos
     2.3.1网关服务引入依赖spring-cloud-starter-alibaba-nacos-discovery
     2.3.2网关服务启动类上加注解@EnableDiscoveryClient
     2.3.3配置文件中配置nacos服务的地址nacos.discovery.server-addr=127.0.0.1:8848
     2.3.4配置文件中开启从nacos中获取信息的开关discovery.locator.enabled=true
     2.3.5浏览器访问http://localhost:8110/order-serv/order/detail?orderId=310，请求成功转发到订单服务(轮询转发到订单服务多个实例)
     2.3.6默认路由，如果配置文件中没有配置cloud.gateway.routes，则会使用默认路由进行请求的转发(第一层路径指定为服务名称)
          浏览器访问http://localhost:8110/order-service/order/detail?orderId=310，请求成功转发到订单服务(轮询转发到订单服务多个实例)
  2.4自定义路由断言 todo
  2.5网关中的过滤器
     2.5.1生命周期
         pre：这种过滤器在请求被路由前之前调用，可以实现身份认证，在集群中选择请求的微服务等
         post：这种过滤器在路由到微服务以后执行，可以实现为响应添加响应头，收集统计信息和指标，将响应从微服务发送给客户端等
     2.5.2分类
          局部过滤器(GatewayFilter)，针对单个路由的过滤器，如网关服务配置文件中的routes.filters属性配置的过滤器
          全局过滤器(GlobalFilter)，应用到所有路由上，可实现权限的统一认证(统一鉴权-验证是否登录)，安全性验证等
     2.5.3内置局部过滤器测试，网关服务配置文件中的routes.filters属性加上以下配置，用来更改响应的状态码
          SetStatus=250
     2.5.4自定义局部过滤器
          2.5.4.1在网关服务配置文件中的routes.filters属性加上Log=true,false，用来控制日志是否开启
          2.5.4.2创建局部过滤器LogGatewayFilterFactory
          2.5.4.3测试，请求网关服务
     2.5.5自定义全局过滤器，实现统一鉴权
          2.5.5.1统一鉴权逻辑，登录请求经过网关时，请求被转发到授权中心微服务，授权中心微服务执行登录逻辑，成功则返回token，后续每次请求携带token经过网关时，
                 网关会进行鉴权处理，鉴权成功则转发到订单等微服务
          2.5.5.2创建授权中心微服务模块auth-service，引入依赖(web,redis等相关依赖)并配置profiles，配置application.yml，application-dev.yml文件(数据库，redis等配置)，引入nacos。
                 创建用户服务微服务模块user-service并配置，提供通过手机号查询用户信息接口，授权中心需要远程调用用户服务
          2.5.5.3网关微服务中创建全局过滤器AuthGlobalFilter，并实现鉴权逻辑
          2.5.5.4postman测试，访问网关地址: http://localhost:8110/login-serv/auth/loginByCode
                                   参数: {"phone": "18312469053","code" :"abc123"}
                               Headers: Content-Type,token
  2.6网关引入限流
     2.6.1引入依赖 todo
     2.6.2添加配置类GatewayConfiguration todo
  2.7整个系统引入链路追踪(Sleuth-用于链路追踪，zipkin-用于收集，存储，查找，展现数据)
     2.7.1概念
          Trace：由一组Trace Id相同的Span串联形成的一个树状结构，为了实现请求跟踪，当请求到达分布式系统的入口端点(比如网关)时，服务跟踪框架会为该请求创建一个
                唯一标识(即TraceId)，该唯一标识会在整个请求链路中流转，标识分布式系统的入口和出口
          Span：代表了一组基本的工作单元,当请求到达具体的微服务时，会创建一个SpanId，标识分布式系统中某个微服务的入口和出口
          Annotation：记录一段时间内的事件
     2.7.2父工程(因为每个微服务都需要追踪)引入依赖spring-cloud-starter-sleuth
     2.7.3postman测试，发送请求http://localhost:8110/login-serv/auth/loginByCode，可在控制台日志中看到以下信息（为什么后面两个服务的服务名称没有显示？？？）
          服务名称          TracId          SpanId    是否展示在第三方平台
         [api-gateway,353bd432852e1ca1,353bd432852e1ca1,false]       网关服务
         [,8823e6fa99124534,a59fb0e24359298d,false]                  授权中心服务
         [,353bd432852e1ca1,138c54d2be1a6f28,false]                  用户服务

         但仅仅这样，不方便看，所以需要引入Zipkin
     2.7.4引入Zipkin，聚合日志，并进行可视化的展示和全文检索(数据默认存储在内存中)
          2.7.4.1官网下载Zipkin的zipkin-server-2.23.16-exec.jar包(服务端)，java条目点击“latest release”下载最新版本，https://zipkin.io/pages/quickstart.html
          2.7.4.2启动Zipkin服务，java -jar zipkin-server-2.23.16-exec.jar
          2.7.4.3浏览器访问http://127.0.0.1:9411/
          2.7.4.4引入Zipkin客户端，在父工程中引入Zipkin的依赖spring-cloud-starter-zipkin
          2.7.4.5每个微服务中配置Zipkin的相关配置(服务端的url，采样百分比等)并重启
          2.7.4.6测试，请求登录接口并在服务端控制台查询链路及对应的服务依赖情况
          2.7.4.7日志中可看到如下信息，最后一项为true，表示展示到第三方平台(Zipkin)
                [api-gateway,1774796e2b52d341,1774796e2b52d341,true]
          2.7.4.8数据持久化到mysql
                 2.7.4.8.1创建zipkin数据库并执行zipkin.sql文件中的sql语句
                 2.7.4.8.2启动Zipkin服务，需要指定数据库的信息
                          java -jar zipkin-server-2.23.16-exec.jar --STORAGE_TYPE=mysql --MYSQL_HOST=127.0.0.1 --MYSQL_TCP_PORT=3306 --MYSQL_DB=zipkin --MYSQL_USER=root --MYSQL_PASS=123456
                 2.7.4.8.3请求登录接口，重启Zipkin服后，查看管理平台是否有对应的数据
          2.7.4.9数据持久化到elasticsearch
                 2.7.4.9.1下载elasticsearch的elasticsearch-8.1.0-windows-x86_64.zip文件，https://www.elastic.co/cn/start
                 2.7.4.9.2解压并启动elasticsearch，双击elasticsearch.bat文件
                 2.7.4.9.3启动Zipkin服务，需要指定elasticsearch的信息
                          java -jar zipkin-server-2.23.16-exec.jar --STORAGE_TYPE=elasticsearch --ES-HOST=localhost:9200
  2.8引入配置中心
     2.8.1搭建nacos服务，1.7步骤已经做了
     2.8.2引入依赖(以订单服务为例)spring-cloud-starter-alibaba-nacos-config
     2.8.3在订单服务中创建bootstrap.yml文件，并配置一些内容(先把application.yml中的内容拷贝到配置中心，然后注释掉application.yml中的内容)
     2.8.4在nacos管理平台中新增配置(order-service-dev.yaml)
          Data ID=bootstrap.yml文件中的服务名+bootstrap.yml文件中的环境名+配置格式类型，如order-service-dev.yaml
     2.8.5获取配置中心中配置的内容
          2.8.5.1硬编码方式，通过ConfigurableApplicationContext获取，ConfigController中的get1方法，postman请求地址http://localhost:9110/config/get1
          2.8.5.1注解方式，通过@Value注解获取，需要配合@RefreshScope注解作动态刷新，ConfigController中的get2方法，postman请求地址http://localhost:9110/config/get2
     2.8.6配置共享
          2.8.6.1同一微服务不同环境之间共享配置，在nacos管理平台中新增配置order-service.yaml(存放各环境公共配置)，order-service-test.yaml，
                                          修改bootstrap.yml文件中的active属性值，postman请求地址http://localhost:9110/config/get2，测试从配置中心获取不同环境的配置内容
          2.8.6.2不同微服务之间共享配置，在nacos管理平台中新增配置all-service.yaml，在bootstrap.yml文件新增shared-dataids和refreshable-dataids配置项
  2.9增加redis+lua脚本扣减库存逻辑，实现见订单服务的创建订单接口
  3.0增加全局唯一id生成器RedisIdWorker
  3.1redis分布式锁
     3.1.1简单实现SimpleRedisLock、SimpleRedisLock2、SimpleRedisLock3
     3.1.2Redisson锁单机版实现(见RedisController类的redissonLockTest方法)
          3.1.2.1引入依赖redisson
          3.1.2.2创建配置类RedissonConfig
          3.1.2.3Redisson实现可重入锁原理，底层使用的是redis中的hash结构set key filed value，filed表示锁标识(用于判断是不是自己的锁)，value表示加锁的次数(每次获取都加1，释放锁都减1)，
                 每次减1时都要重置锁有效期。加锁和解锁的所有的操作都是在各自的lua脚本中完成的(源码中可以看到对应的lua脚本字符串)，保证了原子性。
                 存在单节点宕机风险、主从一致性问题
     3.1.3Redisson联锁实现,每一个Master获取锁成功才算获最终取锁成功(每个Master都是独立的单机实现，原理一样)，解决了主从一致性问题。todo
          3.1.3.1在配置类中定义多个RedissonClient，就端口不一样(多个Master，每个Master可关联多个Slave)
          3.1.3.2调用时获取每个客户端的RLock对象
                  RLock lock1 = redissonClient1.getLock("lock:order:" + userId);
                  RLock lock2 = redissonClient2.getLock("lock:order:" + userId);
                  RLock lock3 = redissonClient3.getLock("lock:order:" + userId);
           3.1.3.3创建联锁、加锁、释放锁
                  RLock lock = redissonClient.getMultiLock(lock,lock2,lock3);
                  lock.tryLock(time,unit);
                  lock.unLock();
  3.2增加redis各种功能(点赞、关注、共同关注、点赞排行榜-最早点赞的N个人、签到)
     3.2.1点赞，利用redis的set集合判断是否点赞过，未点赞过则赞数加1，点赞过则赞数减1。实现见BlogController类的like方法。
               postman测试地址http://localhost:9110/blog/like
                         参数 {
                            	"id":100
                             }
     3.2.2点赞排行榜，实现见BlogController类的like2、likeTop5方法
     3.2.3查关注自己的粉丝，通过SortedSet实现滚动分页(分数设置为当前时间戳，倒序查)，通过分数进行查询，max为上一次的最小分数值(第一次为当前时间戳)，min不变(写为0)，offset为等于max的偏移量的个数(第一次为0，之后为上次最小值的重复数量)，count表示要查询的条数。
          实现见BlogController类的queryBlogOfFollow方法(不完整)
          ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count]
     3.3签到、统计当月连续签到次数，实现分别见BlogController类的sign、countSignNum方法
  3.3引入多级缓存(浏览器缓存+nginx+redis+jvm本地缓存)，解决亿级流量问题
     3.3.1整个架构描述
            浏览器发起请求，先从浏览器缓存中查找数据，如果没找到，则请求会到专门的nginx服务器(专门用来作请求转发)，然后请求会被转发到业务nginx集群(可作缓存，其实是OpenResty集群)，业务nginx集群缓存中
            没有找到数据，则请求会被nginx转发(使用lua脚本)到redis，redis中没有找到数据，则请求会被nginx转发(使用lua脚本)到应用服务器(jvm)，应用服务本地缓存中没有找到数据，最终会从数据库中查找数据。
     3.3.1安装nginx(此nginx专门用作请求转发)
          3.3.1.1下载并解压nginx-1.20.2到D:\wjx http://nginx.org/en/download.html
          3.3.1.2修改nginx配置文件，新增如下配置
                  #业务nginx集群，即OpenResty集群(127.0.0.1:81指向OpenResty)，作nginx本地缓存、redis缓存查询、tomcat缓存查询
                  upstream nginx-cluster{
                      server 127.0.0.1:81;
                  }

                  location /api {
                      proxy_pass http://nginx-cluster;
                  }
          3.3.1.3安装目录下启动、停止nginx
                 start .\nginx.exe

                 快速停止(不保存信息)
                 nginx.exe -s stop

                 有序停止(保存信息)
                 nginx.exe -s quit
          3.3.1.4浏览器访问nginx
                 http://localhost/
     3.3.2应用集成Caffeine本地缓存(goods-service为例)
          3.3.2.1引入依赖caffeine
          3.3.2.2配置缓存配置类CaffeineCacheConfig
          3.3.2.3增加缓存测试方法GoodsController类的selectGoodsFromCaffeine方法
          3.3.2.4测试，浏览器访问http://localhost:9111/goods/detail/caffeine?goodsId=1
                 可以在控制台看到，第一次有sql执行的日志，以后就没有sql执行的日志(缓存有效期内)
     3.3.3安装Lua
          3.3.3.1下载并解压，https://www.lua.org/，点击download->Building中点击get a binary->左侧选中Download->点击lua-5.4.2_Win64_bin.zip进行下载、解压
          3.3.3.2修改解压后文件夹的名称为lua，去掉lua文件夹中所有exe文件名称中的数字
          3.3.3.2配置Path系统环境变量，新增D:\wjx\lua
          3.3.3.4cmd输入lua，出现以下版本信息表示成功
                 Lua 5.4.2  Copyright (C) 1994-2020 Lua.org, PUC-Rio
     3.3.4安装OpenResty
          3.3.4.1下载并解压openresty-1.19.9.1-win64.zip，http://openresty.org/cn/download.html
          3.3.4.2修改配置文件D:\wjx\openresty-1.19.9.1-win64\conf\nginx.conf中的端口为81，避免与nginx监听的端口冲突
          3.3.4.3双击或命令启动nginx，安装目录执行start .\nginx.exe
          3.3.4.4测试，http://localhost:81/
          3.3.4.5修改配置文件
                 在http下面添加对OpenResty的lua模块的加载
                     #加载lua模块
                     lua_package_path "../lualib/?.lua;;";

                     #加载c模块
                     lua_package_path "../lualib/?.so;;";
                 在server下面添加对/api/goods这个路径的监听
                     location /api/goods {#请求入口相当于java中的controller
                         #响应数据类型
                         default_type application/json;
                         #响应数据由goods.lua文件决定
                         content_by_lua_file lua_service/goods.lua;#lua文件相当于java中的service
                     }
          3.3.4.6在安装目录下创建lua_service目录并新建goods.lua文件，内容如下
                 -- 返回假数据，这里的ngx.say()函数就是写数据到Response中
                 ngx.say('{"id":10001,"name":"好东西"}')
          3.3.3.7重载配置文件nginx.exe -s reload
          3.3.3.8测试，http://localhost/api/goods/1，怎么报404？？？
          3.3.3.9OpenResty获取请求参数的方式
                3.3.3.9.1从路径点位符中获取，参数示例/goods/1，正则表达式匹配参数location ~ /goods/(\d+) {content_by_lua_file lua_service/goods.lua;}
                         匹配到的参数会存入到ngx.var数组中，lua文件中可以通过角标获取local id = ngx.var[1]
                3.3.3.9.2从请求头中获取，参数示例id:1000，获取请求头，返回的是tables类型local headers = ngx.req.get_headers()
                3.3.3.9.3从Get请求中获取，参数示例?id=1000，获取Get请求参数，返回的是tables类型ngx.req.get_uri_args()
                3.3.3.9.4从Post表单中获取，参数示例id=1000，先要读取请求体ngx.req.read_body()；然后获取post表单参数，返回的是tables类型local postParams = ngx.req.get_post_args()
                3.3.3.9.5从JSON中获取，参数示例{"id":1000}，先要读取请求体ngx.req.read_body()；然后获取body中的参数，返回的是String类型local jsonBody = ngx.req.get_body_data()
          3.3.3.10修改nginx配置，从路径点位符中获取请求中的商品id参数，将location /api/goods修改为location ~ /api/goods/(\d+)，goods.lua文件中加入local id = ngx.var[1]
          3.3.3.11需求，在lua文件中根据获取到的商品id，向tomcat(商品服务)发送请求以获取商品、库存等信息，最终以json格式返回。
                  3.3.3.11.1原理说明
                             nginx内部提供了API以发送http请求，resp包括resp.status、resp.header、resp.body
                             local resp = ngx.location.capture("/goods",{
                                 method = ngx.HTTP_GET, --请求方式
                                 args = {a=1,b=2},--get方式传参
                                 body = "c=3&d=4" --post方式传参
                             })
                          这里的/goods是路径，并不包括ip和端口，这个请求会被nginx内部的server监听并处理，如果想要这个请求被转发到商品服务，还需要配置代理路径
                              location /goods {
                                  proxy_pass http://localhost:9111;#商品服务的地址
                          }
                  3.3.3.11.2把http查询请求封装为一个函数，放到OpenResty函数库中，方便使用。在\lualib目录下创建common.lua文件并输入以下内容
                            local function read_http(path,params)
                                local resp = ngx.location.capture(path,{
                                                  method = ngx.HTTP_GET, --请求方式
                                                  args = params,--get方式传参
                                              })
                                if not resp then
                                    -- 记录错误信息，返回404
                                    ngx.log(ngx.ERR,"http not found,path:",path,",args:",args)
                                    ngx.exit(404)
                                end
                                return resp.body#成功则返回json字符串结果
                            end

                            -- 将方法导出
                            local _M = {
                                read_http = read_http
                            }
                            return _M
                  3.3.3.11.3在goods.lua文件中导入函数库并发起请求查询商品、库存信息，即加入以下内容
                            local common = require('common') -- require中的common为lua文件的名称
                            local read_http = common.read_http

                            local goodsJson = read_http("/goods/"..id,nil)
                            local stockJson = read_http("/goods/stock"..id,nil) -- todo
                  3.3.3.11.4封装数据并返回，需要导入cjson库，goods.lua文件中加入以下内容
                            local cjson= require('cjson')

                            -- json转为lua中的tables
                            local goods = cjson.decode(goodsJson)
                            local stock = cjson.decode(stockJson)

                            -- 组合数据，库存、销量
                            goods.stock = stock.stock
                            goods.sold = stock.sold

                            -- 将item序列化为json并返回结果，这里的ngx.say()函数就是写数据到Response中
                            ngx.say(cjson.encode(goods))
                  3.3.3.11.5将发向商品服务的请求改为负载均衡请求，在nginx配置文件增加如下内容并修改location中的proxy_pass配置。负载算法需要为$request_uri，让同一请求固定转发到同一个商品服务，否则JVM本地缓存失效
                             upstream goods-cluster {
                                 hash $request_uri;
                                 server localhost:9111;
                                 server localhost:9112;
                               }
     3.3.5redis缓存预热，实现见商品服务RedisHandler类
     3.3.6OpenResty操作redis
          3.3.6.1在lualib/common.lua文件中引入redis模块并初始化redis对象，新增内容如下
                 local redis = require("resty.redis") -- 引入redis模板，resty.redis表示OpenResty中lualib目录下的/resty/redis.lua文件。为什么lualib/resty目录中没有redis.lua文件呢？？？
                 local redisObj = redis:new() -- 调用构造函数创建redis对象
                 redisObj:set_timeouts(1000,1000,1000) -- 设置超时时间，单位：毫秒，分别为与redis建立连接的超时时间、发送请求的超时时间、响应的超时时间
          3.3.6.2封装函数，用来释放redis连接(其实是放入连接池)，在lualib/common.lua文件中新增如下内容
                 local function close_redis(redisObj)
                     local pool_max_idle_time = 10000 -- 连接的空闲时间，单位：毫秒
                     local pool_size = 100 -- 连接池大小
                     local ok,err = redisObj:set_keepalive(pool_max_idle_time,pool_size)
                     if not ok then
                         ngx.log(ngx.ERR,"redis连接释放失败: ",err)
                     end
                 end
          3.3.6.2封装函数，用来从redis读取数据并返回，在lualib/common.lua文件中新增如下内容
                 local function read_redis(ip,port,key)
                     local ok,err = redisObj:connect(ip,port) -- 获取连接
                     if not ok then
                         ngx.log(ngx.ERR,"连接redis失败: ",err)
                         return nil
                     end

                     local resp,err = redisObj:get(key) -- 查询
                     if not resp then -- 查询失败处理
                         ngx.log(ngx.ERR,"查询redis失败: ",err,",key = ",key)
                     end

                     if resp == ngx.null then -- 查询数据为空处理
                         resp = nil
                         ngx.log(ngx.ERR,"查询redis数据为空,key = "，key)
                     end

                     close_redis(redisObj) -- 释放连接
                     return resp
                 end
          3.3.6.3修改goods.lua文件中的查询逻辑，改为先查redis，redis未命中则再查数据库
          3.3.6.4lualib/common.lua文件中的最终内容见nginx_local_cache_common.lua
     3.3.7OpenResty本地缓存(先查OpenResty本地缓存、查不到再查redis缓存、查不到再查应用JVM本地缓存、查不到再查数据库，最终查到后，存入OpenResty本地缓存中)
         3.3.7.1概念，OpenResty为nginx提供了shard dict的功能，即共享字典，可在nginx的多个worker之间共享数据，实现缓存功能
         3.3.7.2开启共享字典，在OpenResty的nginx.conf文件中http下添加如下配置
                lua_shared_dict goods_cache 150m #共享字典，即本地缓存(对象)，名字为goods_cache，大小为150m
         3.3.7.3操作共享字典，在goods.lua文件中导入共享字典
                local goods_cache = ngx.shared.goods_cache -- 获取本地缓存对象
         3.3.7.4修改goods.lua脚本逻辑，改为先从OpenResty本地缓存中查，最终把数据存入本地缓存并设置过期时间。最终内容见nginx_local_cache.lua
                goods_cache:set(key,value,1000) -- 向缓存中存储数据，1000为过期时间，单位：秒，默认为0表示永不过期
                local val = goods_cache:get(key) -- 从缓存中读数据
  3.4缓存同步
     3.4.1缓存数据同步的常见方式
          3.4.1.1设置有效期(OpenResty本地缓存使用的就是此方式)，优势：简单、方便；缺点：时效性较差、缓存过期之前可能不一致；场景：更新频率较低、时效性要求较低的业务
          3.4.1.2同步双写(在修改数据库的同时直接修改缓存)，优势：时效性较强、缓存与数据强一致性；缺点：有代码侵入、耦合度较高；场景：对一致性和时效性要求较高的场景
          3.4.1.2异步通知(修改数据库的时候发送异步通知，相关服务监听到通知后修改缓存数据)，优势：低耦合、可以同时通知多个缓存服务；缺点：时效性一般、可能存在中间不一致状态；场景：时效性要求较高、有多个服务需要同步；
                 实现：MQ中间件或canal(无侵入)，canal的实现见另一个项目的CanalAction类(用的是canal原生API，也可以在SpringBoot中集成canal，简单很多)
  3.5多数据源(一个应用访问多个数据库)
     3.5.1业务场景
          场景一：业务复杂、数据量大，数据分布在不同的数据库，即数据库拆了，但应用没拆
          场景二：读写分离
     3.5.2实现方案 todo
  3.6引入seata
     3.6.1下载seata server服务(当时最新版本1.4.2)并解压到目录D:\wjx\seata\seata-server-1.4.2
          https://seata.io/zh-cn/blog/download.htmls
     3.6.2修改配置文件,将以下配置文件中的注册中心、配置中心由默认的file都改成nacoss
          D:\wjx\seata\seata-server-1.4.2\conf\registry.conf
              A: 文件中nacos的namespace、group配置项目要与客户端保持一致，不然会找不到服务
     3.6.3修改事务会话信息存储方式，将以下配置文件中的mode由默认的file改为db,并修改对应的db配置信息(数据库账号、密码等)。
          D:\wjx\seata\seata-server-1.4.2\conf\file.conf
              A: seata集成了配置中心后，file.conf文件中的内容可以放到配置中心进行统一配置，不用每个微服务都维护一个file.conf文件
              B: 如果配置中心和注册中心的类型都不是file，则不需要file.conf文件了
              C: 如果存储方式为db，则需要创建global_table(全局事务表)、branch_table(分支事务表)、lock_table(全局锁表)三张表，
                 这三张表可以从源码的seata-1.4.2\script\server\db目录中获取(本地下载位置D:\wjx\seata\seata-1.4.2\script\server\db)
                 https://github.com/seata/seata/releasess
              D: file不支持高可用、db支持高可用
     3.6.4启动seata-server(先启动nacos)，双击以下启动文件，启动成功后，可在nacos管理平台中看到服务名为seata-server的服务
          D:\wjx\seata\seata-server-1.4.2\bin\seata-server.bat
     3.6.5修改config.txt配置文件，配置文件内容可从源码目录seata-1.4.2\script\config-center\config.txt中获取(并修改存储方式、事务分组等信息)
          A: 事务分组用于防护机房停电，来启用备用机房的，或是做异地机房容错，如果seata-server端配置了事务分组，client也需要配置相同的事务分组。
             假如现有上海(shanghai)、北京(beijing)两个机房，刚开始使用的是上海机房,事务分组配置如service.vgroupMapping.shanghai=default，一旦上海机房出现了故障，
             则可将事务分组配置改为service.vgroupMapping.beijing=default，即机房由上海切换到了北京
          B: config.txt文件中的配置项service.vgroupMapping.my_test_tx_group=default，其中"my_test_tx_group"可以自定义、"default"为事务分组名称，
             必须要与registry.config文件中的nacos配置项cluster = "default"保持一致
     3.6.6上传config.txt配置文件内容至nacos配置中心(需要修改数据库驱动名称-因为本地数据库版本为8、url中需要加上时区信息，否则启动seata-server报错)
          3.6.6.1上传方式一，通过脚本方式上传
                  3.6.6.1.1将源码目录seata-1.4.2\script\config-center中的config.txt文件拷贝到seata-server安装目录D:\wjx\seata\seata-server-1.4.2
                  3.6.6.1.2将源码目录seata-1.4.2\script\config-center\nacos中的nacos-config.sh脚本文件拷贝到seata-server安装目录D:\wjx\seata\seata-server-1.4.2，
                           修改脚本文件中指定的config.txt文件的路径为当前路径(./)
                  3.6.6.1.3使用git执行以下命令(-t配置命名空间，命名空间为registry.conf文件中配置的命名空间，默认为空)或将nacos-config.sh脚本文件直接拖到git命令窗口
                           sh nacos-config.sh -h localhost -p 8848 -g SEATA_GROUP -t "" -u nacos -w nacos

                           执行结果日志如下
                           set nacosAddr=localhost:8848
                           set group=SEATA_GROUP
                           Set transport.type=TCP successfully
                           Set transport.server=NIO successfully
                           Set transport.heartbeat=true successfully
                           Set transport.enableClientBatchSendRequest=false successfully
                           Set transport.threadFactory.bossThreadPrefix=NettyBoss successfully
                           Set transport.threadFactory.workerThreadPrefix=NettyServerNIOWorker successfully
                           Set transport.threadFactory.serverExecutorThreadPrefix=NettyServerBizHandler successfully
                           Set transport.threadFactory.shareBossWorker=false successfully
                           Set transport.threadFactory.clientSelectorThreadPrefix=NettyClientSelector successfully
                           Set transport.threadFactory.clientSelectorThreadSize=1 successfully
                           Set transport.threadFactory.clientWorkerThreadPrefix=NettyClientWorkerThread successfully
                           Set transport.threadFactory.bossThreadSize=1 successfully
                           Set transport.threadFactory.workerThreadSize=default successfully
                           Set transport.shutdown.wait=3 successfully
                           Set #事务分组用于防护机房停电，来启用备用机房的，或是做异地机房容错，如果seata-server端配置了事务分组，client也需要配置相同的事务分组=#事务分组用于防护机房停电，来启用备用机房的，或是做异地机房容错，如果seata-server端配置了事务分组，client也需要配置相同的事务分组 failure
                           Set service.vgroupMapping.my_test_tx_group=default successfully
                           Set #第二部分的配置"default"就是上一行配置的分组名称=#第二部分的配置"default"就是上一行配置的分组名称 failure
                           Set service.default.grouplist=127.0.0.1:8091 successfully
                           Set service.enableDegrade=false successfully
                           Set service.disableGlobalTransaction=false successfully
                           Set client.rm.asyncCommitBufferLimit=10000 successfully
                           Set client.rm.lock.retryInterval=10 successfully
                           Set client.rm.lock.retryTimes=30 successfully
                           Set client.rm.lock.retryPolicyBranchRollbackOnConflict=true successfully
                           Set client.rm.reportRetryCount=5 successfully
                           Set client.rm.tableMetaCheckEnable=false successfully
                           Set client.rm.tableMetaCheckerInterval=60000 successfully
                           Set client.rm.sqlParserType=druid successfully
                           Set client.rm.reportSuccessEnable=false successfully
                           Set client.rm.sagaBranchRegisterEnable=false successfully
                           Set client.tm.commitRetryCount=5 successfully
                           Set client.tm.rollbackRetryCount=5 successfully
                           Set client.tm.defaultGlobalTransactionTimeout=60000 successfully
                           Set client.tm.degradeCheck=false successfully
                           Set client.tm.degradeCheckAllowTimes=10 successfully
                           Set client.tm.degradeCheckPeriod=2000 successfully
                           Set store.mode=db successfully
                           Set store.publicKey= failure
                           Set store.file.dir=file_store/data successfully
                           Set store.file.maxBranchSessionSize=16384 successfully
                           Set store.file.maxGlobalSessionSize=512 successfully
                           Set store.file.fileWriteBufferCacheSize=16384 successfully
                           Set store.file.flushDiskMode=async successfully
                           Set store.file.sessionReloadReadSize=100 successfully
                           Set store.db.datasource=druid successfully
                           Set store.db.dbType=mysql successfully
                           Set store.db.driverClassName=com.mysql.jdbc.Driver successfully
                           Set store.db.url=jdbc:mysql://127.0.0.1:3306/seata?useUnicode=true&rewriteBatchedStatements=true successfully
                           Set store.db.user=root successfully
                           Set store.db.password=123456 successfully
                           Set store.db.minConn=5 successfully
                           Set store.db.maxConn=30 successfully
                           Set store.db.globalTable=global_table successfully
                           Set store.db.branchTable=branch_table successfully
                           Set store.db.queryLimit=100 successfully
                           Set store.db.lockTable=lock_table successfully
                           Set store.db.maxWait=5000 successfully
                           Set store.redis.mode=single successfully
                           Set store.redis.single.host=127.0.0.1 successfully
                           Set store.redis.single.port=6379 successfully
                           Set store.redis.sentinel.masterName= failure
                           Set store.redis.sentinel.sentinelHosts= failure
                           Set store.redis.maxConn=10 successfully
                           Set store.redis.minConn=1 successfully
                           Set store.redis.maxTotal=100 successfully
                           Set store.redis.database=0 successfully
                           Set store.redis.password= failure
                           Set store.redis.queryLimit=100 successfully
                           Set server.recovery.committingRetryPeriod=1000 successfully
                           Set server.recovery.asynCommittingRetryPeriod=1000 successfully
                           Set server.recovery.rollbackingRetryPeriod=1000 successfully
                           Set server.recovery.timeoutRetryPeriod=1000 successfully
                           Set server.maxCommitRetryTimeout=-1 successfully
                           Set server.maxRollbackRetryTimeout=-1 successfully
                           Set server.rollbackRetryTimeoutUnlockEnable=false successfully
                           Set client.undo.dataValidation=true successfully
                           Set client.undo.logSerialization=jackson successfully
                           Set client.undo.onlyCareUpdateColumns=true successfully
                           Set server.undo.logSaveDays=7 successfully
                           Set server.undo.logDeletePeriod=86400000 successfully
                           Set client.undo.logTable=undo_log successfully
                           Set client.undo.compress.enable=true successfully
                           Set client.undo.compress.type=zip successfully
                           Set client.undo.compress.threshold=64k successfully
                           Set log.exceptionRate=100 successfully
                           Set transport.serialization=seata successfully
                           Set transport.compressor=none successfully
                           Set metrics.enabled=false successfully
                           Set metrics.registryType=compact successfully
                           Set metrics.exporterList=prometheus successfully
                           Set metrics.exporterPrometheusPort=9898 successfully
                           =========================================================================
                            Complete initialization parameters,  total-count:91 ,  failure-count:6
                           =========================================================================
                            init nacos config fail.
                  3.6.6.1.3在nacos管理平台中查看对应的配置信息，可以看到每一个配置项都是单独Data Id，不方便查看
          3.6.6.1上传方式二(1.4.2及以后版本才支持)，直接在nacos管理平台中新建配置，Data Id为seataServer.properties、配置格式选择Properties、拷贝config文件中的内容到配置内容中、点击发布
     3.6.7如果seata-server为集群部署，怎么修改端口？(支持的启动参数还有很多，参考官网)
          Linux/Mac下 sh ./bin/seata-server.sh -p XXX
          Windows下 bin/seata-server.bat -p XXX
     3.6.8在order-service、goods-service服务中增加依赖spring-cloud-starter-alibaba-seata
     3.6.9在对应的服务的数据库中增加undo_log表(用于数据的回滚),可从源码目录seata-1.4.2\script\client\at\db中获取对应数据的undo.log脚本
     3.7.0在order-service服务的配置文件中增加seata相关的配置(配置中心配置seata相关)
          seata:
            tx-service-group: my_test_tx_group #事务分组名称，要和服务端一样，即和配置中心配置项service.vgroupMapping.my_test_tx_group=default中key的第三部分保持一致
            service:
              vgroup-mapping:
                my_test_tx_group: default # key是事务分组名称(上面配置的)，value是配置中心配置项service.vgroupMapping.my_test_tx_group=default中的value，即default
                                          # 也和registry.conf中nacos配置中的cluster的值一样
     3.7.1在goods-service服务的application.yml文件中配置上步中同样的内容(本地配置)
     3.7.2重启seata-server、应用服务等
          如果seata-server存储方式为db则必须先创建3.6.3中的三张表,否则启动seata-server会闪退，seata的C:\Users\xgucs\logs\seata\seata-server.8091.error文件中也不会记录错误日志,
          还是看all文件或warn日志文件，发现以下内容，然后去数据库中创建这三张表，再次重启seata-server，成功启动
          2022-06-20 17:48:01.434  WARN --- [main] i.s.s.s.db.store.LogStoreDataBaseDAO : global_table table or TRANSACTION_NAME column not found
     3.7.3order-service服务OrderController中增加seataTest方法
     3.7.4浏览器测试http://localhost:9110/order/goods/seata/test?orderId=308
          测试时，一直远程接口调不通，版本问题？？？todo

2.如何设置application.yml文件自动提示
  1.1File->Settings->Plugins，搜索spring assistant插件并安装
  1.2File->Settings->Build, Execution & Deployment->Compiler->Annotation Processors，勾选以下选项，确认就可以了(不需要重启idea)
     Enable annotation processing

3.查看Spring Boot、Spring Cloud、Spring Cloud Alibaba版本对应关系
  https://github.com/alibaba/spring-cloud-alibaba/wiki/版本说明

4.疑问
